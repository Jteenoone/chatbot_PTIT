import glob
import os
import shutil
import threading

from certifi import where
from dotenv import load_dotenv

from langchain_community.document_loaders import DirectoryLoader, UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

load_dotenv()
if "OPENAI_API_KEY" not in os.environ:
    print("L·ªói: OPENAI_API_KEY ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p. Vui l√≤ng t·∫°o file .env v√† th√™m key v√†o.")
    exit()

EMBEDDING_MODEL = "text-embedding-3-small"
CHROMA_DB_PATH = "./knowledge_base_ptit"
OLD_DOCS_DIR = "./old_docs"
NEW_DOCS_DIR = "./new_docs"


def load_and_process_documents(docs_dir: str):
    """Load v√† x·ª≠ l√Ω t√†i li·ªáu t·ª´ th∆∞ m·ª•c"""
    try:
        loader = DirectoryLoader(
            docs_dir,
            glob="**/*",
            loader_cls=UnstructuredFileLoader,
            show_progress=True,
            use_multithreading=True
        )
        documents = loader.load()
        if not documents:
            return [], []

        for doc in documents:
            if "source" in doc.metadata:
                doc.metadata["file_name"] = os.path.basename(doc.metadata["source"])

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = text_splitter.split_documents(documents)
        source_files = list(set([doc.metadata.get("source") for doc in documents]))
        return chunks, source_files
    except Exception as e:
        print(f"L·ªói khi load t√†i li·ªáu: {e}")
        return [], []


def initialize_vector_store(db_path: str, embedding_model: str, docs_dir: str):
    """Kh·ªüi t·∫°o ho·∫∑c load Vector Store"""
    try:
        embeddings = OpenAIEmbeddings(model=embedding_model)

        if os.path.exists(db_path):
            # print(f"ƒêang load Knowledge Base t·ª´ '{db_path}'...")
            vector_store = Chroma(
                persist_directory=db_path,
                embedding_function=embeddings
            )
        else:
            # print(f"Knowledge Base ch∆∞a t·ªìn t·∫°i. ƒêang t·∫°o m·ªõi t·ª´ '{docs_dir}'...")
            chunks, _ = load_and_process_documents(docs_dir)

            if not chunks:
                # print("Kh√¥ng c√≥ t√†i li·ªáu ban ƒë·∫ßu, t·∫°o m·ªôt Knowledge Base r·ªóng.")
                vector_store = Chroma(
                    embedding_function=embeddings,
                    persist_directory=db_path
                )
            else:
                # print(f"ƒêang embedding {len(chunks)} chunks...")
                vector_store = Chroma.from_documents(
                    chunks,
                    embedding=embeddings,
                    persist_directory=db_path
                )

            # vector_store.persist()
            # print(f"ƒê√£ t·∫°o v√† l∆∞u Knowledge Base v√†o '{db_path}'.")

        return vector_store
    except Exception as e:
        print(f"L·ªói khi kh·ªüi t·∫°o Vector Store: {e}")
        exit(1)


def check_and_update_database(vector_store: Chroma, new_docs_dir: str, old_docs_dir: str):
    """Ki·ªÉm tra v√† c·∫≠p nh·∫≠t database v·ªõi t√†i li·ªáu m·ªõi"""
    try:
        if not os.path.exists(new_docs_dir) or not os.listdir(new_docs_dir):
            print("Th∆∞ m·ª•c 'new_docs' r·ªóng, kh√¥ng c√≥ g√¨ ƒë·ªÉ c·∫≠p nh·∫≠t.")
            return

        new_chunks, processed_files = load_and_process_documents(new_docs_dir)
        if not new_chunks:
            print("Kh√¥ng t√¨m th·∫•y t√†i li·ªáu m·ªõi h·ª£p l·ªá.")
            return

        # L·∫•y t√™n file t·ª´ metadata
        new_file_names = set()
        for chunk in new_chunks:
            if "file_name" in chunk.metadata:
                new_file_names.add(chunk.metadata["file_name"])

        old_files = [
            os.path.basename(f)
            for f in glob.glob(os.path.join(old_docs_dir, "**", "*.*"), recursive=True)
        ]

        existing = [f for f in old_files if f in new_file_names]

        if existing:
            print("\nB·∫°n mu·ªën c·∫≠p nh·∫≠t nh·ªØng file sau:")
            for f in existing:
                print(f"  - {f}")
            confirm = input("\nNh·∫≠p 0 ƒë·ªÉ h·ªßy b·ªè, Nh·∫≠p Enter ƒë·ªÉ ti·∫øp t·ª•c: ")
            if confirm.strip() == "0":
                print("ƒê√£ h·ªßy b·ªè c·∫≠p nh·∫≠t.")
                return

            # X√≥a c√°c file c≈© t·ª´ vector store
            for chunk in new_chunks:
                if "file_name" in chunk.metadata and chunk.metadata["file_name"] in existing:
                    vector_store.delete(
                        where={"file_name": chunk.metadata["file_name"]}
                    )
            print(f"ƒê√£ x√≥a {len(existing)} file c≈© kh·ªèi Knowledge Base.")

        print(f"\nƒêang th√™m {len(new_chunks)} chunks m·ªõi v√†o Knowledge Base...")
        vector_store.add_documents(new_chunks)
        # vector_store.persist()

        # Di chuy·ªÉn file t·ª´ new_docs sang old_docs
        for file_path in processed_files:
            file_name = os.path.basename(file_path)
            destination_path = os.path.join(old_docs_dir, file_name)

            if os.path.exists(destination_path):
                os.remove(destination_path)  # X√≥a b·∫£n c≈©

            shutil.move(file_path, destination_path)

        print(f"ƒê√£ di chuy·ªÉn {len(processed_files)} file sang 'old_docs'.")
        print("--- Ho√†n t·∫•t quy tr√¨nh c·∫≠p nh·∫≠t ---")

    except Exception as e:
        print(f"L·ªói khi c·∫≠p nh·∫≠t database: {e}")

update_lock = threading.Lock()

def update_knowledge_base_auto():
    if update_lock.locked():
        print("[Auto Update] üöß ƒêang c·∫≠p nh·∫≠t, vui l√≤ng ƒë·ª£i!")
        return {"success": False, "message": "ƒêang c√≥ t√°c v·ª• c·∫≠p nh·∫≠t kh√°c."}

    with update_lock:  # Ch·ªâ 1 update ƒë∆∞·ª£c ph√©p ch·∫°y
        try:
            print("\n[Auto Update] B·∫Øt ƒë·∫ßu c·∫≠p nh·∫≠t tri th·ª©c...")

            os.makedirs(OLD_DOCS_DIR, exist_ok=True)
            os.makedirs(NEW_DOCS_DIR, exist_ok=True)

            # Load DB (kh√¥ng t·∫°o m·ªõi n·∫øu c√≥ s·∫µn)
            db = initialize_vector_store(CHROMA_DB_PATH, EMBEDDING_MODEL, OLD_DOCS_DIR)

            # Update t·ª´ th∆∞ m·ª•c new_docs
            check_and_update_database(db, NEW_DOCS_DIR, OLD_DOCS_DIR)

            print("[Auto Update] Ho√†n t·∫•t c·∫≠p nh·∫≠t tri th·ª©c.")
            return {"success": True, "message": "C·∫≠p nh·∫≠t th√†nh c√¥ng"}

        except Exception as e:
            print(f"[Auto Update] L·ªói: {e}")
            return {"success": False, "message": str(e)}


if __name__ == "__main__":
    print("=== B·∫Øt ƒë·∫ßu qu√° tr√¨nh x√¢y d·ª±ng/c·∫≠p nh·∫≠t Knowledge Base ===\n")

    os.makedirs(OLD_DOCS_DIR, exist_ok=True)
    os.makedirs(NEW_DOCS_DIR, exist_ok=True)

    # 1. Kh·ªüi t·∫°o ho·∫∑c t·∫£i Vector Store
    db = initialize_vector_store(CHROMA_DB_PATH, EMBEDDING_MODEL, OLD_DOCS_DIR)

    # 2. Ki·ªÉm tra v√† c·∫≠p nh·∫≠t t·ª´ th∆∞ m·ª•c new_docs
    check_and_update_database(db, NEW_DOCS_DIR, OLD_DOCS_DIR)

    print("\n=== Ho√†n t·∫•t ===")