import os
import json
import shutil
import threading
import time
from datetime import datetime
from dotenv import load_dotenv

from langchain_community.document_loaders import UnstructuredFileLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

# ==============================
# ‚öôÔ∏è C·∫•u h√¨nh
# ==============================
load_dotenv()
EMBEDDING_MODEL = "text-embedding-3-small"
CHROMA_DB_PATH = "./knowledge_base_ptit"
OLD_DOCS_DIR = "./old_docs"
UPDATE_LOG_FILE = "./update_log.json"

update_lock = threading.Lock()
_vector_cache = None
chatbot_reload_callback = None  # Flask s·∫Ω g√°n callback reload chatbot


# ==============================
# üîπ X·ª≠ l√Ω & nh√∫ng t√†i li·ªáu
# ==============================
def process_file(file_path):
    """ƒê·ªçc & chia nh·ªè n·ªôi dung file."""
    try:
        loader = UnstructuredFileLoader(file_path)
        docs = loader.load()

        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = splitter.split_documents(docs)

        for c in chunks:
            c.metadata["file_name"] = os.path.basename(file_path)
        return chunks
    except Exception as e:
        print(f"[‚ùå] L·ªói khi x·ª≠ l√Ω {file_path}: {e}")
        return []


# ==============================
# üîπ Kh·ªüi t·∫°o ho·∫∑c t·∫£i vector store
# ==============================
def get_vector_store():
    """T·∫°o ho·∫∑c load vector store cache."""
    global _vector_cache
    if _vector_cache is None:
        embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)
        _vector_cache = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings)
    return _vector_cache


# ==============================
# üîπ Ghi log c·∫≠p nh·∫≠t
# ==============================
def log_update(file_name, status, chunks, duration):
    entry = {
        "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "file": file_name,
        "status": status,
        "chunks": chunks,
        "duration_sec": round(duration, 2)
    }
    with open(UPDATE_LOG_FILE, "a", encoding="utf-8") as f:
        json.dump(entry, f, ensure_ascii=False)
        f.write("\n")


def get_update_logs(limit=10):
    if not os.path.exists(UPDATE_LOG_FILE):
        return []
    with open(UPDATE_LOG_FILE, "r", encoding="utf-8") as f:
        lines = f.readlines()[-limit:]
        return [json.loads(l) for l in lines if l.strip()]


# ==============================
# üîπ X√≥a tri th·ª©c theo file
# ==============================
def delete_knowledge(file_name):
    """X√≥a file & vector t∆∞∆°ng ·ª©ng"""
    try:
        embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)
        store = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings)
        store.delete(where={"file_name": file_name})

        old_path = os.path.join(OLD_DOCS_DIR, file_name)
        if os.path.exists(old_path):
            os.remove(old_path)

        print(f"[üóëÔ∏è] ƒê√£ x√≥a tri th·ª©c: {file_name}")
        return True
    except Exception as e:
        print(f"[‚ùå] L·ªói khi x√≥a {file_name}: {e}")
        return False


# ==============================
# üîπ Th√™m ho·∫∑c c·∫≠p nh·∫≠t tri th·ª©c
# ==============================
def add_or_update_file(file_path, force_replace=False):
    """
    N·∫øu file ƒë√£ t·ªìn t·∫°i:
        - N·∫øu force_replace=True => x√≥a file + vector c≈©, r·ªìi th√™m m·ªõi.
        - N·∫øu False => tr·∫£ v·ªÅ c·∫£nh b√°o.
    """
    os.makedirs(OLD_DOCS_DIR, exist_ok=True)
    file_name = os.path.basename(file_path)
    old_path = os.path.join(OLD_DOCS_DIR, file_name)

    # N·∫øu file t·ªìn t·∫°i m√† ch∆∞a ch·ªçn ghi ƒë√®
    if os.path.exists(old_path) and not force_replace:
        return {"exists": True, "file": file_name}

    start = time.time()
    try:
        store = get_vector_store()
        chunks = process_file(file_path)

        if not chunks:
            return {"success": False, "message": f"Kh√¥ng th·ªÉ x·ª≠ l√Ω {file_name}"}

        # N·∫øu ghi ƒë√®: x√≥a vector + file c≈©
        if os.path.exists(old_path):
            delete_knowledge(file_name)

        # Th√™m t√†i li·ªáu m·ªõi
        store.add_documents(chunks)
        shutil.copy(file_path, old_path)

        duration = time.time() - start
        log_update(file_name, "success", len(chunks), duration)

        # Reload chatbot sau khi c·∫≠p nh·∫≠t
        if chatbot_reload_callback:
            chatbot_reload_callback()

        return {
            "success": True,
            "message": f"‚úÖ ƒê√£ th√™m/c·∫≠p nh·∫≠t {file_name} ({len(chunks)} ƒëo·∫°n)."
        }
    except Exception as e:
        log_update(file_name, "error", 0, 0)
        return {"success": False, "message": str(e)}


# ==============================
# üîπ T·ª± ƒë·ªông c·∫≠p nh·∫≠t n·ªÅn (n·∫øu c·∫ßn)
# ==============================

def start_auto_update(interval=7200):
    """C·∫≠p nh·∫≠t t·ª± ƒë·ªông m·ªói interval gi√¢y (n·∫øu mu·ªën)."""
    def loop():
        while True:
            try:
                print("[‚è≥] Ki·ªÉm tra c·∫≠p nh·∫≠t tri th·ª©c ƒë·ªãnh k·ª≥...")
                time.sleep(interval)
            except KeyboardInterrupt:
                break
    threading.Thread(target=loop, daemon=True).start()


# ==============================
# üîπ Khi ch·∫°y tr·ª±c ti·∫øp
# ==============================
if __name__ == "__main__":
    os.makedirs(OLD_DOCS_DIR, exist_ok=True)
    print("[üí°] H·ªá th·ªëng tri th·ª©c PTIT s·∫µn s√†ng.")
